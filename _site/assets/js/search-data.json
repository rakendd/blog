{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "http://0.0.0.0:4000/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Decision Tree from the Scratch",
            "content": "Python algorithm built from the scratch for a simple Decision Tree. . Theory and Math behind this post is covered here in Medium post. . We have just looked at Mathematical working for ID3, this post we will see how to build this in Python from the scratch. We will make it simple by using Pandas dataframes. . Load the prerequisites . import numpy as np import pandas as pd eps = np.finfo(float).eps from numpy import log2 as log . ‘eps’ here is the smallest representable number. At times we get log(0) or 0 in the denominator, to avoid that we are going to use this. . The dataset is very small one, we’ll just represent that with a dictionary. . dataset = {&#39;Taste&#39;:[&#39;Salty&#39;,&#39;Spicy&#39;,&#39;Spicy&#39;,&#39;Spicy&#39;,&#39;Spicy&#39;,&#39;Sweet&#39;,&#39;Salty&#39;,&#39;Sweet&#39;,&#39;Spicy&#39;,&#39;Salty&#39;], &#39;Temperature&#39;:[&#39;Hot&#39;,&#39;Hot&#39;,&#39;Hot&#39;,&#39;Cold&#39;,&#39;Hot&#39;,&#39;Cold&#39;,&#39;Cold&#39;,&#39;Hot&#39;,&#39;Cold&#39;,&#39;Hot&#39;], &#39;Texture&#39;:[&#39;Soft&#39;,&#39;Soft&#39;,&#39;Hard&#39;,&#39;Hard&#39;,&#39;Hard&#39;,&#39;Soft&#39;,&#39;Soft&#39;,&#39;Soft&#39;,&#39;Soft&#39;,&#39;Hard&#39;], &#39;Eat&#39;:[&#39;No&#39;,&#39;No&#39;,&#39;Yes&#39;,&#39;No&#39;,&#39;Yes&#39;,&#39;Yes&#39;,&#39;No&#39;,&#39;Yes&#39;,&#39;Yes&#39;,&#39;Yes&#39;]} . We will read this with Pandas dataframe . df = pd.DataFrame(dataset,columns=[&#39;Taste&#39;,&#39;Temperature&#39;,&#39;Texture&#39;,&#39;Eat&#39;]) . df . Taste Temperature Texture Eat . 0 Salty | Hot | Soft | No | . 1 Spicy | Hot | Soft | No | . 2 Spicy | Hot | Hard | Yes | . 3 Spicy | Cold | Hard | No | . 4 Spicy | Hot | Hard | Yes | . 5 Sweet | Cold | Soft | Yes | . 6 Salty | Cold | Soft | No | . 7 Sweet | Hot | Soft | Yes | . 8 Spicy | Cold | Soft | Yes | . 9 Salty | Hot | Hard | Yes | . We have seen from an earlier post we need to find the Entropy and then Information Gain for splitting the data set. . $S = - sum limits_{i=1}^N p_i log_2 p_i$ . Here the fraction is $p_i$, it is the proportion of a number of elements in that split group to the number of elements in the group before splitting(parent group). . We’ll define a function that takes in class (target variable vector) and finds the entropy of that class. . entropy_node = 0 #Initialize Entropy values = df.Eat.unique() #Unique objects - &#39;Yes&#39;, &#39;No&#39; for value in values: fraction = df.Eat.value_counts()[value]/len(df.Eat) entropy_node += -fraction*np.log2(fraction) . Summation is taken care of by ‘entropy +=’ . We can check this by supplying our data frame to this function . entropy_node . 0.9709505944546686 . This is same as the entropy $(E_o)$ we calculated in the theory post. Now the entropy of the attribute Taste . attribute = &#39;Taste&#39; target_variables = df.Eat.unique() #This gives all &#39;Yes&#39; and &#39;No&#39; variables = df[attribute].unique() #This gives different features in that attribute (like &#39;Sweet&#39;) entropy_attribute = 0 for variable in variables: entropy_each_feature = 0 for target_variable in target_variables: num = len(df[attribute][df[attribute]==variable][df.Eat ==target_variable]) #numerator den = len(df[attribute][df[attribute]==variable]) #denominator fraction = num/(den+eps) #pi entropy_each_feature += -fraction*log(fraction+eps) #This calculates entropy for one feature like &#39;Sweet&#39; fraction2 = den/len(df) entropy_attribute += -fraction2*entropy_each_feature #Sums up all the entropy ETaste . abs(entropy_attribute) . 0.7609640474436806 . This is same as the entropy $E_{Taste}$ we calculated in the theory post . Now the Information Gain is simply . $IG_{Taste} = entropy_{node} — entropy_{attribute} = 0.21$ . We will continue this for the other attributes ‘Temperature’ and ‘Texture’. . We just need to replace attribute= ‘Taste’ with ‘Temperature’ and ‘Texture’ . We get, . $E_{Temperature} = 0.9509$ . $E_{Texture} = 0.9245$ . Then Information Gain, . $IG_{Temperature} = 0.02$ . $IG_{Texture} = 0.05$ . Next process: . We’ll find the winner node, the one with the highest Information Gain. We repeat this process to find which is the attribute we need to consider to split the data at the nodes. . We build a decision tree based on this. Below is the complete code. . def find_entropy(df): Class = df.keys()[-1] #To make the code generic, changing target variable class name entropy = 0 values = df[Class].unique() for value in values: fraction = df[Class].value_counts()[value]/len(df[Class]) entropy += -fraction*np.log2(fraction) return entropy def find_entropy_attribute(df,attribute): Class = df.keys()[-1] #To make the code generic, changing target variable class name target_variables = df[Class].unique() #This gives all &#39;Yes&#39; and &#39;No&#39; variables = df[attribute].unique() #This gives different features in that attribute (like &#39;Hot&#39;,&#39;Cold&#39; in Temperature) entropy2 = 0 for variable in variables: entropy = 0 for target_variable in target_variables: num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable]) den = len(df[attribute][df[attribute]==variable]) fraction = num/(den+eps) entropy += -fraction*log(fraction+eps) fraction2 = den/len(df) entropy2 += -fraction2*entropy return abs(entropy2) def find_winner(df): Entropy_att = [] IG = [] for key in df.keys()[:-1]: # Entropy_att.append(find_entropy_attribute(df,key)) IG.append(find_entropy(df)-find_entropy_attribute(df,key)) return df.keys()[:-1][np.argmax(IG)] def get_subtable(df, node,value): return df[df[node] == value].reset_index(drop=True) def buildTree(df,tree=None): Class = df.keys()[-1] #To make the code generic, changing target variable class name #Here we build our decision tree #Get attribute with maximum information gain node = find_winner(df) #Get distinct value of that attribute e.g Salary is node and Low,Med and High are values attValue = np.unique(df[node]) #Create an empty dictionary to create tree if tree is None: tree={} tree[node] = {} #We make loop to construct a tree by calling this function recursively. #In this we check if the subset is pure and stops if it is pure. for value in attValue: subtable = get_subtable(df,node,value) clValue,counts = np.unique(subtable[&#39;Eat&#39;],return_counts=True) if len(counts)==1:#Checking purity of subset tree[node][value] = clValue[0] else: tree[node][value] = buildTree(subtable) #Calling the function recursively return tree . Now we call the buildTree function and print the tree we built. . tree = buildTree(df) . import pprint pprint.pprint(tree) . {&#39;Taste&#39;: {&#39;Salty&#39;: {&#39;Texture&#39;: {&#39;Hard&#39;: &#39;Yes&#39;, &#39;Soft&#39;: &#39;No&#39;}}, &#39;Spicy&#39;: {&#39;Temperature&#39;: {&#39;Cold&#39;: {&#39;Texture&#39;: {&#39;Hard&#39;: &#39;No&#39;, &#39;Soft&#39;: &#39;Yes&#39;}}, &#39;Hot&#39;: {&#39;Texture&#39;: {&#39;Hard&#39;: &#39;Yes&#39;, &#39;Soft&#39;: &#39;No&#39;}}}}, &#39;Sweet&#39;: &#39;Yes&#39;}} . You can see the tree structure is formed based on the priority list to split the data. . We can write an algorithm to predict using this tree structure. . def predict(inst,tree): #This function is used to predict for any input variable #Recursively we go through the tree that we built earlier for nodes in tree.keys(): value = inst[nodes] tree = tree[nodes][value] prediction = 0 if type(tree) is dict: prediction = predict(inst, tree) else: prediction = tree break; return prediction . Prediction . inst = df.iloc[6] #This takes row with index 6 . inst . Taste Salty Temperature Cold Texture Soft Eat No Name: 6, dtype: object . We can check with our available data. . We took a row with index 6 from our dataframe to use this for our prediction. . Get prediction . data = {&#39;Taste&#39;:&#39;Salty&#39;, &#39;Temperature&#39;:&#39;Cold&#39;, &#39;Texture&#39;:&#39;Hard&#39;} . inst = pd.Series(data) . prediction = predict(inst, tree) prediction . &#39;Yes&#39; . Our tree predicted kid will eat this food (which is Salty, Cold and Hard). . This is a simple tree building algorithm without much control parameters. . Some of the contents here, I am inspired from this book ‘Ensemble machine learning-Ankit Dixit’. I encourage you to read it for further explorations. . Below cell is for styling, ignore it. . from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;&quot;)) .",
            "url": "http://0.0.0.0:4000/machine%20learning/2018/01/15/Decision_trees_from_scratch.html",
            "relUrl": "/machine%20learning/2018/01/15/Decision_trees_from_scratch.html",
            "date": " • Jan 15, 2018"
        }
        
    
  
    
  
    
        ,"post3": {
            "title": "Linear Regression",
            "content": "In the current post, we will try to understand simple linear regression algorithm and its algorithm writing from scratch and same thing we compare that comes from sci-kit learn And some of the statistical terminologies to understand the model. For this, we&#39;ll use Boston Housing Data set, this is a sample dataset from sklearn . Import required Python libraries . import numpy as np import pandas as pd import sklearn import matplotlib.pyplot as plt from matplotlib import animation, rc from IPython.display import HTML . # Load the data set from sklearn.datasets import load_boston . boston_data = load_boston() . type(boston_data) . sklearn.utils.Bunch . boston_data is a dictionary, like a regular Python dictionary we can access its keys and values . boston_data.keys() . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;feature_names&#39;, &#39;DESCR&#39;]) . To check the features . boston_data[&#39;feature_names&#39;] . array([&#39;CRIM&#39;, &#39;ZN&#39;, &#39;INDUS&#39;, &#39;CHAS&#39;, &#39;NOX&#39;, &#39;RM&#39;, &#39;AGE&#39;, &#39;DIS&#39;, &#39;RAD&#39;, &#39;TAX&#39;, &#39;PTRATIO&#39;, &#39;B&#39;, &#39;LSTAT&#39;], dtype=&#39;&lt;U7&#39;) . To check the size of data . boston_data[&#39;data&#39;].shape . (506, 13) . And there is description about the data . print(boston_data[&#39;DESCR&#39;]) . Boston House Prices dataset =========================== Notes Data Set Characteristics: :Number of Instances: 506 :Number of Attributes: 13 numeric/categorical predictive :Median Value (attribute 14) is usually the target :Attribute Information (in order): - CRIM per capita crime rate by town - ZN proportion of residential land zoned for lots over 25,000 sq.ft. - INDUS proportion of non-retail business acres per town - CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) - NOX nitric oxides concentration (parts per 10 million) - RM average number of rooms per dwelling - AGE proportion of owner-occupied units built prior to 1940 - DIS weighted distances to five Boston employment centres - RAD index of accessibility to radial highways - TAX full-value property-tax rate per $10,000 - PTRATIO pupil-teacher ratio by town - B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town - LSTAT % lower status of the population - MEDV Median value of owner-occupied homes in $1000&#39;s :Missing Attribute Values: None :Creator: Harrison, D. and Rubinfeld, D.L. This is a copy of UCI ML housing dataset. http://archive.ics.uci.edu/ml/datasets/Housing This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The Boston house-price data of Harrison, D. and Rubinfeld, D.L. &#39;Hedonic prices and the demand for clean air&#39;, J. Environ. Economics &amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics ...&#39;, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter. The Boston house-price data has been used in many machine learning papers that address regression problems. **References** - Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&#39;, Wiley, 1980. 244-261. - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing) . We will create Pandas DataFrame with data from Boson dataset . df = pd.DataFrame(data=boston_data[&#39;data&#39;]) df.columns = boston_data[&#39;feature_names&#39;] . We&#39; ll add target data to this DataFrame . df[&#39;Price&#39;] = boston_data[&#39;target&#39;] . df.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT Price . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . We&#39;ll first try Simple Linear Regression with single independent variable . If we check the correlation of all other features with Target, . corr = df.corr() . corr[&#39;Price&#39;].sort_values(ascending=False) . Price 1.000000 RM 0.695360 ZN 0.360445 B 0.333461 DIS 0.249929 CHAS 0.175260 AGE -0.376955 RAD -0.381626 CRIM -0.385832 NOX -0.427321 TAX -0.468536 INDUS -0.483725 PTRATIO -0.507787 LSTAT -0.737663 Name: Price, dtype: float64 . From description RM is number of rooms per dwelling. This feature is more correlated with housing price. . We can also qualitatively see the correlation map . import seaborn as sns f, ax = plt.subplots(figsize=(10, 8)) sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax) plt.show() . The bottom most row show the correlation the square . from pylab import rcParams rcParams[&#39;figure.figsize&#39;] = 20, 15 . import matplotlib.pyplot as plt import pandas as pd import numpy as np from pandas.plotting import scatter_matrix axes = scatter_matrix(df, alpha=0.5, diagonal=&#39;kde&#39;) corr = df.corr().as_matrix() for i, j in zip(*plt.np.triu_indices_from(axes, k=1)): axes[i, j].annotate(&quot;%.3f&quot; %corr[i,j], (0.8, 0.8), xycoords=&#39;axes fraction&#39;, ha=&#39;center&#39;, va=&#39;center&#39;) plt.show() . From the last column we can see only the RM feature has good linear relation with housing price So, for this Simple Linear Regression we&#39;ll just consider RM and Price features . x = df[&#39;RM&#39;] y = df[&#39;Price&#39;] . from pylab import rcParams rcParams[&#39;figure.figsize&#39;] = 10, 8 . plt.scatter(x,y) plt.xlabel(&#39;Number of rooms per house&#39;, size = 20) plt.ylabel(&#39;House Price&#39;, size = 20) plt.show() . We need to do feature scaling for linear gradient, for better convergence . x = (x - x.mean())/x.std() y = (y - y.mean())/y.std() . plt.scatter(x,y) plt.xlabel(&#39;Number of rooms per house&#39;, size = 20) plt.ylabel(&#39;House Price&#39;, size = 20) plt.show() . Simple Linear Regression . This is called simple linear regression model since it involves only one regressor variable (RM) And we want to model the data using a linear plot . begin{equation} y = theta_0 + theta_1 x end{equation} Here x is the independent variable (or predictor or regressor) &#39;RM&#39; and y is the dependent variable(or response) &#39;House Price&#39; . We use Stochastic Gradient Descent (or Batch Gradient Descent) to find these coefficients . Gradient Descent . This is an optimization technique to find the minimum or maximum of a function by iterating through a sequence of steps. Gradient Descent algorithms also called steepest descent algorithms . Parameters $ theta_0$ and $ theta_1$ are the regression coefficients Slope $ theta_1$ is the change in the mean of the distribution of y produced by a unit change in x . begin{equation} y^i = theta_0 + theta_1 x^i, quad quad i = 1,2,3...m end{equation} We need to find a fitted line for this. Say, the simple linear regression model is . begin{equation} widehat{y^i} = widehat{ theta_0} + widehat{ theta_1} x^i end{equation} where least square estimators of $ theta_0$ and $ theta_1$ are $ widehat{ theta_0}$ and $ widehat{ theta_1} $ . Linear regression models the relation between the regressor and response with a linear surface called hyperplane. This hyperplane has one dimension less than the dimension of the dataset. For simple linear regression, we are dealing with two dimensions, x and y. So, hyperplane here will be 1D - line . $ widehat{y}$ linearly depends on only two parameters $ theta_0$ and $ theta_1$, so it lies within a one dimensional subspace of the 2-dimensional space in which $y$ points lies. So, predictions of a linear regression are always on a straight line. . . The difference between the observed value $y^i$ and the corresponding fitted value $ widehat{y^i}$ is a residual . $i^{th} $ residual is begin{equation} e^i = y^i - widehat{y^i}, quad i = 1,2,...m end{equation} . begin{equation} e^i = y^i - ( widehat{ theta_0} + widehat{ theta_1} x^i) end{equation} So the predicted value begin{equation} widehat{y^i} = widehat{ theta_0} + widehat{ theta_1} x^i end{equation} . i.e. say for first point $x^1$ correponding predicted y point is $ widehat{y_1}$ $$ widehat{y_1} = widehat{ theta_0} + widehat{ theta_1} x^1 $$ Similarly $$ widehat{y_2} = widehat{ theta_0} + widehat{ theta_1} x^2 $$ $$ . $$ $$ . $$ $$ . $$ $$ widehat{y_m} = widehat{ theta_0} + widehat{ theta_1} x^m $$ . begin{equation} widehat{y^1} + widehat{y^2} + ... widehat{y^m} = m times widehat{ theta_0} + widehat{ theta_1} times (x^1 + x^2 +... x^m) end{equation} We want find the estimators $ widehat{ theta_0}$ and $ widehat{ theta_1}$ such that above LHS and RHS are equal . From equation 21 and 32 we can write Total error $$ epsilon = m times widehat{ theta_0} + widehat{ theta_1} times (x^1 + x^2 +... x^m) - y^1 + y^2 + .... y^m $$ . We can rewrite this as $$ epsilon = h_{ theta}(x^i) - y^i $$ . where $h_{ theta}(x^i)$ is called hypothesis function. Since, here we are using a linear function as hypothesis function, even if the actual data exhibits some non-linearity, the mathematical model we form will be a linear one, and the nonlinearlity in the data will be treated as noise or residue. . In fact, in linear regression model, we assume the errors have mean zero and unknown variance . Mean square error is $$ dfrac{1}{m} sum limits_{i=1}^m [h_{ theta}(x^i) - y^i]^2 $$ . In Gradient descent we are using method of least squares to estimate these parameters, so the sum of the squares of the differences between the observations y and the straight line is minimum . Mean of squared error or Cost function begin{equation} J( theta_0, theta_1) = dfrac{1}{2m} sum limits_{i=1}^m [h_{ theta}(x^i) - y^i ]^2 end{equation} . Dividing by m to get the mean of the squared errors, where m is the total number of samples considered 2 in the denominator, we are just using for convention, so when we take the differentiation of the J, 2 from the square gets cancelled. . Cost function helps us in understanding how good or bad the model predicts the relation between x and y . We need to estimate $ theta_0$ and $ theta_1$ such that J is minimum . For this we use Gradient Descent technique. This is an optimization algorithm that is used to estimate the local minimum of a function. . Our function is residual sum of squares : cost function. In this technique we will first make some guess or start from some location and from there we&#39;ll descent to the function minimum location going in the direction of the maximum descent. Maximum descent direction is obtained by finding the gradient of the function at that point. . Sometimes gradient descent land us to local minimum point, but fortunately the function we are considering residual sum of squares has no local minium, i.e it is a convex function. . . As per this technique we first estimate some values for $ theta_0$ and $ theta_1$ . We&#39;ll first start with initial guess for $ theta_0$ and $ theta_1$ and update the values Gradient descent algorithm . $$( theta_0, theta_1)&#39; = ( theta_0, theta_1) - alpha nabla J( theta_0, theta_1)$$ . $ alpha$ is the learning rate. We keep this constant through out the algorithm. $ nabla J$ is the gradient of the cost function J. The distance we move depends on this gradient. i.e. distance we move is proportional to the steepness of the function at that position. So we take big steps when the function is steep and small steps when it is shallow. . $$ nabla J = dfrac{ partial J}{ partial theta_0} mathbf{i} + dfrac{ partial J}{ partial theta_0} mathbf{j} $$ . where i and j are unit vectors in $ theta_0$ and $ theta_1$ direction respectively . Taking partial derivatives of the cost function . begin{equation} dfrac{ partial J}{ partial theta_0} = dfrac{1}{m} sum limits_{i=1}^m ( widehat{ theta_0} + widehat{ theta_1} x^i - y^i) end{equation} begin{equation} dfrac{ partial J}{ partial theta_1} = dfrac{1}{m} sum limits_{i=1}^m ( widehat{ theta_0} + widehat{ theta_1} x^i - y^i)x^i end{equation} By plugging these to update $ theta_0$ and $ theta_1$ $$( theta_0, theta_1)&#39; = ( theta_0, theta_1) - alpha dfrac{1}{m} big[ sum limits_{i=1}^m ( widehat{ theta_0} + widehat{ theta_1} x^i - y^i) mathbf{i} + sum limits_{i=1}^m ( widehat{ theta_0} + widehat{ theta_1} x^i - y^i)x^i mathbf{j} big]$$ . To simplify the expression, say $x_0$ = 1 begin{equation} ( theta_0, theta_1)&#39; = ( theta_0, theta_1) - alpha dfrac{1}{m} big[ sum limits_{i=1}^m ( widehat{ theta_0} + widehat{ theta_1} x^i - y^i) x_0^i mathbf{i} + sum limits_{i=1}^m ( widehat{ theta_0} + widehat{ theta_1} x^i - y^i)x_1^i mathbf{j} big] end{equation} . $$( theta_0, theta_1)&#39; = ( theta_0, theta_1) - alpha dfrac{1}{m} sum limits_{i=1}^m ( widehat{ theta_0} + widehat{ theta_1} x^i - y^i) x_j^i $$ . Here i is for the $i^{th}$ sample j in subscript means $j^{th}$ feature. So the above expression works for even multiple linear regression problem where the number of features we are dealing with are more than one. . In this post we&#39;ll try to solve Gradient Descent from the scratch and compare that with the sci-kit learn Gradient Descent model . From the scratch . So as per this equation we need to consider $x_0$ as ones i.e. we need to add a column of ones to x . x = np.c_[np.ones(x.shape[0]),x] . # Parameters required for Gradient Descent alpha = 0.0001 #learning rate m = y.size #no. of samples np.random.seed(10) theta = np.random.rand(2) #initializing theta with some random values . def gradient_descent(x, y, m, theta, alpha): cost_list = [] #to record all cost values to this list theta_list = [] #to record all theta_0 and theta_1 values to this list prediction_list = [] run = True cost_list.append(1e10) #we append some large value to the cost list i=0 while run: prediction = np.dot(x, theta) #predicted y values theta_0*x0+theta_1*x1 prediction_list.append(prediction) error = prediction - y cost = 1/(2*m) * np.dot(error.T, error) # (1/2m)*sum[(error)^2] cost_list.append(cost) theta = theta - (alpha * (1/m) * np.dot(x.T, error)) # alpha * (1/m) * sum[error*x] theta_list.append(theta) if cost_list[i]-cost_list[i+1] &lt; 1e-9: #checking if the change in cost function is less than 10^(-9) run = False i+=1 cost_list.pop(0) # Remove the large number we added in the begining return prediction_list, cost_list, theta_list . prediction_list, cost_list, theta_list = gradient_descent(x, y, m, theta, alpha) theta = theta_list[-1] . print(&quot;Values of theta are {:2f} and {:2f}&quot;.format(theta[0], theta[1])) . Values of theta are 0.002370 and 0.693263 . plt.title(&#39;Cost Function J&#39;, size = 30) plt.xlabel(&#39;No. of iterations&#39;, size=20) plt.ylabel(&#39;Cost&#39;, size=20) plt.plot(cost_list) plt.show() . Animating Cost Function . Now we&#39;ll try to see the cost function animation i.e. how cost function value eventually change with the number of iterations and respectively how the linear regression line change . Just ignore the below code, it is too much hard coded for making the animation. . xl = np.linspace(0, len(cost_list),len(cost_list)) cost_list_mod = cost_list[::-1][0::100][::-1] x_mod = xl[::-1][0::100][::-1] prediction_list_mod = prediction_list[::-1][0::100][::-1] . fig = plt.figure(figsize=(12,10)) ax1=plt.subplot(121) ax1.scatter(x[:,1], y, color=&#39;C1&#39;) ax1.set_xlabel(&#39;Average number of rooms per dwelling&#39;) ax1.set_ylabel(&#39;House Price&#39;) ax2=plt.subplot(122) ax2.set_xlim(-2,140000) ax2.set_ylim((0, 1)) ax2.set_xlabel(&#39;Number of iterations&#39;) ax2.set_ylabel(&#39;Cost Function&#39;) line1, = ax1.plot([], [], lw=2); line2, = ax2.plot([], [], lw=2); line3, = ax2.plot([], [], &#39;o&#39;); annotation1 = ax1.text(-3.5, 3, &#39;&#39;, size=18) annotation2 = ax2.text(16500, .957, &#39;&#39;, size=18) annotation1.set_animated(True) annotation2.set_animated(True) plt.close() . def init(): line1.set_data([], []) line2.set_data([], []) return (line1, line2,) # animation function. def animate(i): line1.set_data(x[:,1], prediction_list_mod[i]) line2.set_data(x_mod[:i],cost_list_mod[:i]) line3.set_data(x_mod[i],cost_list_mod[i]) annotation1.set_text(&#39;J = %.2f&#39; % (cost_list_mod[i])) annotation2.set_text(&#39;J = %.2f&#39; % (cost_list_mod[i])) return line1, line2, line3, annotation1, annotation2 anim = animation.FuncAnimation(fig, animate, init_func=init, frames=500,interval=15, blit=True) anim.save(&#39;animation.gif&#39;, writer=&#39;imagemagick&#39;, fps = 30) # HTML(anim.to_html5_video()) . #Display the animation... import io import base64 from IPython.display import HTML filename = &#39;animation.gif&#39; video = io.open(filename, &#39;r+b&#39;).read() encoded = base64.b64encode(video) HTML(data=&#39;&#39;&#39;&lt;img src=&quot;data:image/gif;base64,{0}&quot; type=&quot;gif&quot; /&gt;&#39;&#39;&#39;.format(encoded.decode(&#39;ascii&#39;))) . import matplotlib.animation as animation print(animation.writers.list()) . [&#39;ffmpeg&#39;, &#39;ffmpeg_file&#39;, &#39;imagemagick&#39;, &#39;imagemagick_file&#39;] . Ok, from the above animation we notice that with the number of iterations increasing the cost function value is decreasing and the linear regression line is fitting properly to the data. . Since, for this $ alpha$ (learning rate) this case is converging, so this is happening. For other $ alpha$ like increasing, this won&#39;t be happening. We&#39;ll try to plot the cost function for different $ alpha$ and see how alpha affects the convergence . alpha_list = [1,0.5,0.1,0.05,0.01] plot_style = [&quot;-&quot;,&quot;--&quot;,&quot;-.&quot;,&quot;:&quot;] from itertools import cycle plot_styles = cycle(plot_style) for i in range(len(alpha_list)): prediction_list, cost_list, theta_list = gradient_descent(x, y, m, theta, alpha_list[i]) plt.title(&#39;Cost Function J&#39;, size = 30) plt.xlabel(&#39;No. of iterations&#39;, size=20) plt.ylabel(&#39;Cost&#39;, size=20) plt.plot(cost_list,next(plot_styles), label =r&#39;$ alpha$ = {}&#39;.format(alpha_list[i]) ) plt.legend() plt.show() . We can see with the increase in $ alpha$ the curve steep rate is increasing, so the convergence rate will be higher . Residuals are very useful in finding the fidelity of the model . Mean square of the residuals can be find by . Mean square of residuals = $(1/m) times (Predicted quad Response - Response)^2 $ . Predicted value we can either find from $y_{predicted} = theta_0 + theta_1 times x$ . yp = theta[1]*x[:,1] + theta[0] . round(((yp - y) ** 2).mean(),4) . 0.5155 . Or from the gradient_descent function we found the prediction list . round(((prediction_list[-1] - y)**2).mean(),4) . 0.5155 . Variance . Variance is the measure of how far the data set values are spread out. If all the house prices are equal, then variance of the dataset is zero . $var(x) = dfrac{ sum_{i=1}^{n}(x_i - bar{x})^2 }{n-1}$ . var = sum((x[:,1] - x[:,1].mean())**2) *(1/(m-1)) round(var,3) . 1.0 . We can also use numpy way for calculating this . round(np.var(x[:,1], ddof=1),3) . 1.0 . Covariance . Covariance is a measure of how much two variables change together If the values increase together, then covariance is positive. If one value increases and other decreases then covariance is negative. If there is no linear relationship then covariance is zero . $cov(x,y) = dfrac{ sum_{i=1}^{n}(x_i - bar{x}) (y_i - bar{y}) }{n-1}$ . cov = sum((x[:,1] - x[:,1].mean()) * (y - y.mean()) )*(1/(m-1)) print(round(cov,3)) . 0.695 . We can also do numpy way to do this . np.cov(x[:,1],y) . array([[ 1. , 0.69535995], [ 0.69535995, 1. ]]) . np.cov actually gives a covariance matrix The diagonal terms gives variance i.e. $ dfrac{ sum_{i=1}^{n}(x_i - bar{x})^2 }{n-1}$ and off-diagonal terms gives covariance i.e. $ dfrac{ sum_{i=1}^{n}(x_i - bar{x}) (y_i - bar{y}) }{n-1}$ . So, covariance here is . print(round(np.cov(x[:,1],y)[0][1],3)) . 0.695 . $ theta_0$ and $ theta_1$ values are also related as $ bar{y} = theta_0 + theta_1 times bar{x}$ . Since we know the variance and covariance of the dataset, we can calculate $ theta_1$ from them $ theta_1 = dfrac{cov(x,y)}{var(x)}$ . theta_1 = cov/var theta_0 = y.mean() - theta_1 * x[:,1].mean() print(&#39;theta_0 = {} and theta_1 = {} &#39;.format(round(theta_0,3),round(theta_1,3))) . theta_0 = 0.0 and theta_1 = 0.695 . # And from gradient descent we got print(&#39;theta_0 = {} and theta_1 = {} &#39;.format(round(theta[0],3),round(theta[1],3))) . theta_0 = 0.002 and theta_1 = 0.693 . residuals = abs(y - prediction_list[-1]) var = abs(x[:,1] - x[:,1].mean()) covar = abs((x[:,1] - x[:,1].mean())*(y - y.mean())) . sc=plt.scatter(x[:,1],y, s=30*residuals, c = covar,alpha=0.5) plt.plot(x[:,1],prediction_list[-1],color=&#39;C1&#39;) fig = plt.gcf() fig.set_size_inches(15, 10) plt.xlabel(&#39;Average number of rooms per dwelling&#39;, size=15) plt.ylabel(&#39;House Price&#39;, size=15) s1 = plt.scatter([],[], s=1*30, marker=&#39;o&#39;, color=&#39;#555555&#39;) s2 = plt.scatter([],[], s=2.5*30, marker=&#39;o&#39;, color=&#39;#555555&#39;) s3 = plt.scatter([],[], s=5*30, marker=&#39;o&#39;, color=&#39;#555555&#39;) plt.legend((s1,s2,s3), (&#39;1&#39;, &#39;2.5&#39;,&#39;5&#39;), scatterpoints=1, loc=&#39;lower right&#39;, ncol=3, fontsize=12, ) cb=plt.colorbar(sc) cb.set_label(&#39;Covariance of data points&#39;, fontsize=20) plt.text(2, -2.4, &#39;Residual size&#39;,fontsize=15) plt.show() . This is just a fancy plot same as earlier one, but showing the size of data point with respect to its residual and color with respect to its covariance. So, larger size data points are far away from the prediction line and has large error in estimation. . Using sci-kit learn . from sklearn.linear_model import LinearRegression from sklearn import preprocessing . lm = LinearRegression() . xs = df[&#39;RM&#39;] ys = df[&#39;Price&#39;] . # Feature scaling xs = preprocessing.scale(xs) ys = preprocessing.scale(ys) . xs = xs.reshape(-1,1) ys = ys.reshape(-1,1) . lm.fit(xs,ys) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) . pred = lm.predict(xs) . plt.scatter(xs,ys, label=&#39;Data&#39;) plt.plot(xs,pred, color=&#39;C1&#39;,label=&#39;Linear Regression&#39;) plt.xlabel(&#39;Number of rooms per house&#39;, size = 20) plt.ylabel(&#39;House Price&#39;, size = 20) plt.legend(prop={&#39;size&#39;: 15}) plt.title(&#39;Using sci-kit learn&#39;, size=30) plt.show() . # To get the intercept and slope of the line print(round(lm.intercept_[0],3),&#39;,&#39;, round(lm.coef_[0][0],3)) . -0.0 , 0.695 . These are $ theta_0$ and $ theta_1$ we calculated using gradient descent from scratch . print(round(theta[0],3),round(theta[1],3)) . 0.002 0.693 . $R^2$ . r-squared describes the proportion of variance in the response variable predicted by the model . $R^2 = 1 - dfrac{ sum_{i=1}^{n} (y_i - y_{predicted})^2}{ sum_{i=1}^{n} (y_i - bar{y})^2}$ . r2 = 1 - (sum((y - prediction_list[-1])**2)) / (sum((y - y.mean())**2)) . print(round(r2,3)) . 0.484 . From sci-kit learn we can calculate same thing as . print(round(lm.score(xs,ys),3)) . 0.484 . And mean squared error from sci-kit learn . from sklearn.metrics import mean_squared_error . mean_squared_error(ys,pred) . 0.51647454400866588 . Below cells are for styling, ignore them. . %%javascript MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } } }); Below cell is for styling, ignore it . from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;&quot;)) .",
            "url": "http://0.0.0.0:4000/machine%20learning/2018/01/03/linear_regression.html",
            "relUrl": "/machine%20learning/2018/01/03/linear_regression.html",
            "date": " • Jan 3, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My areas of interest include Machine Learning, Artifical Intelligence and Reinforcement Learning. So typically my posts will be revolving around these topics that inspire me. I love sharing ideas, thoughts and contributing to Open Source community. Typically the posts are concocted from my rough worksheets while working on the fundamentals. Hope you find something interesting/useful here. Some times I write in Medium . _ . This website is powered by fastpages",
          "url": "http://0.0.0.0:4000/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://0.0.0.0:4000/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}